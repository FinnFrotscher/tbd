{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc28N0_NrCQH"
      },
      "source": [
        "## â—¢ Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import argparse, os, sys, glob, cv2\n",
        "from os import path\n",
        "from base64 import b64encode\n",
        "from PIL import Image, ImageDraw\n",
        "from IPython.display import HTML\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import autocast\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "from diffusers import StableDiffusionPipeline, AutoencoderKL\n",
        "from diffusers import UNet2DConditionModel, PNDMScheduler, LMSDiscreteScheduler\n",
        "from diffusers.schedulers.scheduling_ddim import DDIMScheduler\n",
        "\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# from omegaconf import OmegaConf\n",
        "# from tqdm import tqdm, trange\n",
        "# from itertools import islice\n",
        "# from einops import rearrange, repeat\n",
        "# from torchvision.utils import make_grid\n",
        "# from contextlib import nullcontext\n",
        "# import time\n",
        "# from pytorch_lightning import seed_everything\n",
        "\n",
        "# from ldm.util import instantiate_from_config\n",
        "# from ldm.models.diffusion.ddim import DDIMSampler\n",
        "# from ldm.models.diffusion.plms import PLMSSampler\n",
        "# from ldm.dream.devices         import choose_torch_device\n",
        "\n",
        "\n",
        "device = 'cuda'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "cwd = path.join(os.getcwd())\n",
        "modelpath = 'models/ldm/stable-diffusion-v1/model.ckpt'\n",
        "loadpath = path.normpath(path.join(cwd, '..', modelpath))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "vae = AutoencoderKL.from_pretrained(loadpath)\n",
        "vae"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
