{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NHgUAp48qwoG",
    "outputId": "411d4df6-d91a-42d4-819e-9cf641c12248"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/finn/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse, os, sys, glob, cv2\n",
    "from os import path\n",
    "from base64 import b64encode\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import HTML\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import autocast, float16\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from diffusers import StableDiffusionPipeline, AutoencoderKL\n",
    "from diffusers import UNet2DConditionModel, PNDMScheduler, LMSDiscreteScheduler\n",
    "from diffusers.schedulers.scheduling_ddim import DDIMScheduler\n",
    "\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from base64 import b64encode\n",
    "# from IPython.display import HTML\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "cwd = path.join(os.getcwd())\n",
    "modelpath = 'models/ldm/stable-diffusion-v1'\n",
    "loadpath = path.normpath(path.join(cwd, '..', modelpath))\n",
    "\n",
    "# print(loadpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cNHvQBhzyXCI",
    "outputId": "0a79e979-8484-4c62-96d9-7c79b1835162"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1. Load the autoencoder model which will be used to decode the latents into image space. \n",
    "vae = AutoencoderKL.from_pretrained('/home/finn/data/stable-diffusion-v1-4/vae')\n",
    "vae.to(device)\n",
    "\n",
    "# 2. Load the tokenizer and text encoder to tokenize and encode the text. \n",
    "tokenizer = CLIPTokenizer.from_pretrained('/home/finn/data/stable-diffusion-v1-4/tokenizer')\n",
    "text_encoder = CLIPTextModel.from_pretrained('/home/finn/data/stable-diffusion-v1-4/text_encoder')\n",
    "text_encoder.to(device)\n",
    "\n",
    "# 3. The UNet model for generating the latents.\n",
    "unet = UNet2DConditionModel.from_pretrained('/home/finn/data/stable-diffusion-v1-4/unet')\n",
    "unet.to(device)\n",
    "\n",
    "\n",
    "# 4. Create a scheduler for inference\n",
    "scheduler = LMSDiscreteScheduler(\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule='scaled_linear',\n",
    "    num_train_timesteps=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jbL2zJ7Pt7Jl",
    "outputId": "c8242be9-dba2-4a9f-da44-a294a70bb449"
   },
   "outputs": [],
   "source": [
    "def get_text_embeds(prompt):\n",
    "  # Tokenize text and get embeddings\n",
    "  text_input = tokenizer(\n",
    "      prompt, \n",
    "      padding='max_length', \n",
    "      max_length=tokenizer.model_max_length,\n",
    "      truncation=True, \n",
    "      return_tensors='pt'\n",
    "    )\n",
    "\n",
    "  with torch.no_grad():\n",
    "    text_embeddings = text_encoder(text_input.input_ids.to(device))[0]\n",
    "\n",
    "  # Do the same for unconditional embeddings\n",
    "  uncond_input = tokenizer(\n",
    "      [''] * len(prompt), padding='max_length',\n",
    "      max_length=tokenizer.model_max_length, return_tensors='pt')\n",
    "  with torch.no_grad():\n",
    "    uncond_embeddings = text_encoder(uncond_input.input_ids.to(device))[0]\n",
    "\n",
    "  # Cat for final embeddings\n",
    "  text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "  return text_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "id": "fnGwQRhtyBhb"
   },
   "outputs": [],
   "source": [
    "def produce_latents(text_embeddings, height=512, width=512,\n",
    "                    num_inference_steps=50, guidance_scale=7.5, latents=None):\n",
    "  if latents is None:\n",
    "    latents = torch.randn((text_embeddings.shape[0] // 2, unet.in_channels, \\\n",
    "                           height // 8, width // 8))\n",
    "  latents = latents.to(device)\n",
    "\n",
    "  scheduler.set_timesteps(num_inference_steps)\n",
    "  latents = latents * scheduler.sigmas[0]\n",
    "\n",
    "  with autocast('cuda'):\n",
    "    for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "      # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "      latent_model_input = torch.cat([latents] * 2)\n",
    "      sigma = scheduler.sigmas[i]\n",
    "      latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
    "\n",
    "      # predict the noise residual\n",
    "      with torch.no_grad():\n",
    "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)['sample']\n",
    "\n",
    "      # perform guidance\n",
    "      noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "      noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "      # compute the previous noisy sample x_t -> x_t-1\n",
    "      latents = scheduler.step(noise_pred, i, latents)['prev_sample']\n",
    "  \n",
    "  return latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BPnyd-XUKbfE",
    "outputId": "0fcd10e4-0df2-4ab9-cbf5-f08f4902c954"
   },
   "outputs": [],
   "source": [
    "def decode_img_latents(latents):\n",
    "  latents = 1 / 0.18215 * latents\n",
    "\n",
    "  with torch.no_grad():\n",
    "    imgs = vae.decode(latents).sample\n",
    "\n",
    "  imgs = (imgs + 0.5).clamp(0, 1)\n",
    "  imgs = imgs.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "  imgs = (imgs * 255).round().astype('uint8')\n",
    "  pil_images = [Image.fromarray(image) for image in imgs]\n",
    "  return pil_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prompt_to_image(prompts, height=512, width=512, num_inference_steps=50, guidance_scale=7.5, latents=None):\n",
    "    if isinstance(prompts, str):\n",
    "        prompts = [prompts]\n",
    "\n",
    "    text_embeds = get_text_embeds(prompts)\n",
    "    latents = produce_latents(\n",
    "        text_embeds,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        latents=latents,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale\n",
    "    )\n",
    "\n",
    "    imgs = decode_img_latents(latents)\n",
    "    return imgs\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "latent-imagenet-diffusion.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('ldm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "452f1c6e8cb0f44ff4ad9fc1fa62290003f481fd297f9c1d55eb35f5a5fe9020"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
